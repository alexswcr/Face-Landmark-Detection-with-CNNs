{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexswcr/Face-Landmark-Detection-with-CNNs/blob/Add-Files/Task_2_Face_Alignment_Pt_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AtRTXleZAFh"
      },
      "source": [
        "# Face Alignment NoteBook 2: Creating Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTDljjP_5CXc"
      },
      "source": [
        "**To run these codeblocks, please replace the file path in np.load() to your directory containing the respective file, the code submission zip folder should contain a copy of each file needed**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daz5XPLS0Mht"
      },
      "source": [
        "This notebook shows how each model was created, this was made into a separate notebook due to RAM running out on the first notebook, meaning all codeblocks could not be run in excession.\n",
        "\n",
        "Instead, the preprocessed data was saved into a .npz file, so that it could be loaded in this notebook and used to train the model.\n",
        "\n",
        "Each model is set to run for 15 epochs, which takes around 7-8hrs, I would not advise trying to run multiple of the 'Creating Model x' code blocks in the same runtime as the RAM will likely run out. This is why each code block has repeated code, they are designed to be run independently (after this first code block to get the training data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoPTSFHoZaJ0"
      },
      "source": [
        "## Splitting training data 90/10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9T6brDNRfP0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "\n",
        "drive.mount(\"/content/gdrive/\")\n",
        "\n",
        "#Calculated mean landmark distance for a batch\n",
        "@keras.saving.register_keras_serializable(package=\"my_package\", name=\"landmark_distance\")\n",
        "def landmark_distance(y_true,y_pred):\n",
        "\n",
        "  distance = tf.sqrt(tf.reduce_sum(tf.square(y_true-y_pred),-1))\n",
        "\n",
        "  return tf.reduce_mean(distance)\n",
        "\n",
        "\n",
        "#Load training Data\n",
        "transform_data = np.load('/content/gdrive/MyDrive/Colab-Notebooks/AML-coursework/PROCESSED_DATA_FINAL.npz',allow_pickle=True)\n",
        "x = transform_data['x_train']\n",
        "y = transform_data['y_train']\n",
        "\n",
        "#Split training data\n",
        "train_test_split = int(np.round(x.shape[0]*0.9))\n",
        "x_train = x[:train_test_split]\n",
        "y_train = y[:train_test_split]\n",
        "x_test = x[train_test_split:]\n",
        "y_test = y[train_test_split:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtCv2HVwZuK5"
      },
      "source": [
        "## Creating Model 1\n",
        "Please do not run the commented code as this will overwrite the trained model and replace it with a new one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWuvMasfZZ01",
        "outputId": "d19a81f6-f53b-4bf1-ea00-15ef8e5ea713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{-45: 0.7071067811865476, -44: 0.7193398003386512, -43: 0.7313537016191705, -42: 0.7431448254773942, -41: 0.754709580222772, -40: 0.766044443118978, -39: 0.7771459614569709, -38: 0.7880107536067219, -37: 0.7986355100472928, -36: 0.8090169943749475, -35: 0.8191520442889918, -34: 0.8290375725550416, -33: 0.838670567945424, -32: 0.848048096156426, -31: 0.8571673007021123, -30: 0.8660254037844387, -29: 0.8746197071393957, -28: 0.882947592858927, -27: 0.8910065241883679, -26: 0.898794046299167, -25: 0.9063077870366499, -24: 0.9135454576426009, -23: 0.9205048534524404, -22: 0.9271838545667874, -21: 0.9335804264972017, -20: 0.9396926207859084, -19: 0.9455185755993168, -18: 0.9510565162951535, -17: 0.9563047559630354, -16: 0.9612616959383189, -15: 0.9659258262890683, -14: 0.9702957262759965, -13: 0.9743700647852352, -12: 0.9781476007338057, -11: 0.981627183447664, -10: 0.984807753012208, -9: 0.9876883405951378, -8: 0.9902680687415704, -7: 0.992546151641322, -6: 0.9945218953682733, -5: 0.9961946980917455, -4: 0.9975640502598242, -3: 0.9986295347545738, -2: 0.9993908270190958, -1: 0.9998476951563913, 0: 1.0, 1: 0.9998476951563913, 2: 0.9993908270190958, 3: 0.9986295347545738, 4: 0.9975640502598242, 5: 0.9961946980917455, 6: 0.9945218953682733, 7: 0.992546151641322, 8: 0.9902680687415704, 9: 0.9876883405951378, 10: 0.984807753012208, 11: 0.981627183447664, 12: 0.9781476007338057, 13: 0.9743700647852352, 14: 0.9702957262759965, 15: 0.9659258262890683, 16: 0.9612616959383189, 17: 0.9563047559630354, 18: 0.9510565162951535, 19: 0.9455185755993168, 20: 0.9396926207859084, 21: 0.9335804264972017, 22: 0.9271838545667874, 23: 0.9205048534524404, 24: 0.9135454576426009, 25: 0.9063077870366499, 26: 0.898794046299167, 27: 0.8910065241883679, 28: 0.882947592858927, 29: 0.8746197071393957, 30: 0.8660254037844387, 31: 0.8571673007021123, 32: 0.848048096156426, 33: 0.838670567945424, 34: 0.8290375725550416, 35: 0.8191520442889918, 36: 0.8090169943749475, 37: 0.7986355100472928, 38: 0.7880107536067219, 39: 0.7771459614569709, 40: 0.766044443118978, 41: 0.754709580222772, 42: 0.7431448254773942, 43: 0.7313537016191705, 44: 0.7193398003386512}\n"
          ]
        }
      ],
      "source": [
        "#Size of an image with batch size 1\n",
        "input_shape = (180,180,1)\n",
        "\n",
        "#Defining the network\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        #Set input shape\n",
        "        keras.Input(shape = input_shape),\n",
        "        #Convolutional layers and pooling layers, convolutional layer uses relu activation\n",
        "        layers.Conv2D(64, kernel_size=(3, 3),kernel_initializer='he_normal', activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3),kernel_initializer='he_normal', activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(128, kernel_size=(3, 3),kernel_initializer='he_normal', activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(128, kernel_size=(3, 3),kernel_initializer='he_normal', activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        #Flatten outputs\n",
        "        layers.Flatten(),\n",
        "        #Dropout to reduce overfitting\n",
        "        layers.Dropout(0.2),\n",
        "        #Produce 10 outputs\n",
        "        layers.Dense(10),\n",
        "        #Reshape outputs\n",
        "        layers.Reshape((5,2))\n",
        "    ]\n",
        ")\n",
        "#Show model output sizes and parameters at each layer\n",
        "model.summary()\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 15\n",
        "\n",
        "#Compile model with loss and optimizer function, showing accuracy and mean landmark_distance\n",
        "model.compile(loss = \"mean_squared_error\",optimizer=\"adam\",metrics= [\"accuracy\",landmark_distance])\n",
        "\n",
        "#Save model with the highest validation accuracy \n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=\"/content/gdrive/MyDrive/Colab-Notebooks/AML-coursework/Best_CNN_model_New_Copy.keras\",monitor=\"val_accuracy\",mode=\"max\",save_best_only=True)\n",
        "\n",
        "#GUI to show training performance and progress\n",
        "tensorBoard = keras.callbacks.TensorBoard(\n",
        "    log_dir=\"/content/gdrive/MyDrive/Colab-Notebooks/AML-coursework/CNN_Model_Logs\",\n",
        "    histogram_freq=0,\n",
        "    embeddings_freq=0,\n",
        "    update_freq=\"epoch\",\n",
        ")\n",
        "#Save some RAM with garbage collection\n",
        "gc.collect()\n",
        "\n",
        "#Load tensorBoard\n",
        "%tensorboard --logdir logs\n",
        "\n",
        "#Train model on training data\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1,callbacks=(model_checkpoint,tensorBoard))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8KxtqOdzeUb"
      },
      "source": [
        "## Creating Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAmveIvPzTlF"
      },
      "outputs": [],
      "source": [
        "#Pretty much identical to Model 1 except for a different number of layers\n",
        "input_shape = (180,180,1)\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape = input_shape),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3),kernel_initializer='he_normal', activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(128, kernel_size=(3, 3),kernel_initializer='he_normal', activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(10),\n",
        "        layers.Reshape((5,2))\n",
        "    ]\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 15\n",
        "\n",
        "model.compile(loss = \"mean_squared_error\",optimizer=\"adam\",metrics= [\"accuracy\",landmark_distance])\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=\"/content/gdrive/MyDrive/Colab-Notebooks/AML-coursework/CNN_Model_2_Reduced_layers.keras\",monitor=\"val_accuracy\",mode=\"max\",save_best_only=True)\n",
        "\n",
        "tensorBoard = keras.callbacks.TensorBoard(\n",
        "    log_dir=\"/content/gdrive/MyDrive/Colab-Notebooks/AML-coursework/CNN_Model_Logs\",\n",
        "    histogram_freq=0,\n",
        "    embeddings_freq=0,\n",
        "    update_freq=\"epoch\",\n",
        ")\n",
        "gc.collect()\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1,callbacks=(model_checkpoint,tensorBoard))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRPLSeBhzhLK"
      },
      "source": [
        "##Creating Model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olAB65tlzkg2"
      },
      "outputs": [],
      "source": [
        "#Pretty much identical to Model 1 except for a different number of filters\n",
        "input_shape = (180,180,1)\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape = input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3),kernel_initializer='he_normal', activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3),kernel_initializer='he_normal', activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3),kernel_initializer='he_normal', activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3),kernel_initializer='he_normal', activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(10),\n",
        "        layers.Reshape((5,2))\n",
        "    ]\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 15\n",
        "\n",
        "model.compile(loss = \"mean_squared_error\",optimizer=\"adam\",metrics= [\"accuracy\",landmark_distance])\n",
        "model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=\"/content/gdrive/MyDrive/Colab-Notebooks/AML-coursework/CNN_Model_3_Reduced_filters.keras\",monitor=\"val_accuracy\",mode=\"max\",save_best_only=True)\n",
        "\n",
        "tensorBoard = keras.callbacks.TensorBoard(\n",
        "    log_dir=\"/content/gdrive/MyDrive/Colab-Notebooks/AML-coursework/CNN_Model_Logs\",\n",
        "    histogram_freq=0,\n",
        "    embeddings_freq=0,\n",
        "    update_freq=\"epoch\",\n",
        ")\n",
        "gc.collect()\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1,callbacks=(model_checkpoint,tensorBoard))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM1DrDVCXBY7Qs7Dk8EDcgL",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
